       PyTorch Layer Name              PyTorch Shape          PyTorch Type            | Keras Layer Name                Keras Shape            Keras Type
1.     input                        -> (1, 3, 224, 224)       (Type: Input)           | input_layer                  -> (1, 224, 224, 3)       (Type: InputLayer)      
2.     conv1                        -> (1, 64, 112, 112)      (Type: Conv2d)          | conv1_conv                   -> (1, 112, 112, 64)      (Type: Conv2D)          
3.     bn1                          -> (1, 64, 112, 112)      (Type: BatchNorm2d)     | conv1_bn                     -> (1, 112, 112, 64)      (Type: BatchNormalization)
4.     relu                         -> (1, 64, 112, 112)      (Type: ReLU)            | conv1_relu                   -> (1, 112, 112, 64)      (Type: Activation)      
5.     maxpool                      -> (1, 64, 56, 56)        (Type: MaxPool2d)       | pool1_pool                   -> (1, 56, 56, 64)        (Type: MaxPooling2D)    
6.     layer1.0.conv1               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block1_1_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
7.     layer1.0.bn1                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block1_1_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
8.     layer1.0.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block1_1_relu          -> (1, 56, 56, 64)        (Type: Activation)      
9.     layer1.0.conv2               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block1_2_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
10.    layer1.0.bn2                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block1_2_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
11.    layer1.0.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block1_2_relu          -> (1, 56, 56, 64)        (Type: Activation)      
12.    layer1.0.conv3               -> (1, 256, 56, 56)       (Type: Conv2d)          | conv2_block1_3_conv          -> (1, 56, 56, 256)       (Type: Conv2D)          
13.    layer1.0.bn3                 -> (1, 256, 56, 56)       (Type: BatchNorm2d)     | conv2_block1_3_bn            -> (1, 56, 56, 256)       (Type: BatchNormalization)
14.    layer1.0.downsample.0        -> (1, 256, 56, 56)       (Type: Conv2d)          | conv2_block1_0_conv          -> (1, 56, 56, 256)       (Type: Conv2D)          
15.    layer1.0.downsample.1        -> (1, 256, 56, 56)       (Type: BatchNorm2d)     | conv2_block1_0_bn            -> (1, 56, 56, 256)       (Type: BatchNormalization)
16.    layer1.0.relu                -> (1, 256, 56, 56)       (Type: ReLU)            | conv2_block1_out             -> (1, 56, 56, 256)       (Type: Activation)      
17.    layer1.1.conv1               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block2_1_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
18.    layer1.1.bn1                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block2_1_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
19.    layer1.1.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block2_1_relu          -> (1, 56, 56, 64)        (Type: Activation)      
20.    layer1.1.conv2               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block2_2_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
21.    layer1.1.bn2                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block2_2_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
22.    layer1.1.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block2_2_relu          -> (1, 56, 56, 64)        (Type: Activation)      
23.    layer1.1.conv3               -> (1, 256, 56, 56)       (Type: Conv2d)          | conv2_block2_3_conv          -> (1, 56, 56, 256)       (Type: Conv2D)          
24.    layer1.1.bn3                 -> (1, 256, 56, 56)       (Type: BatchNorm2d)     | conv2_block2_3_bn            -> (1, 56, 56, 256)       (Type: BatchNormalization)
25.    layer1.1.relu                -> (1, 256, 56, 56)       (Type: ReLU)            | conv2_block2_out             -> (1, 56, 56, 256)       (Type: Activation)      
26.    layer1.2.conv1               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block3_1_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
27.    layer1.2.bn1                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block3_1_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
28.    layer1.2.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block3_1_relu          -> (1, 56, 56, 64)        (Type: Activation)      
29.    layer1.2.conv2               -> (1, 64, 56, 56)        (Type: Conv2d)          | conv2_block3_2_conv          -> (1, 56, 56, 64)        (Type: Conv2D)          
30.    layer1.2.bn2                 -> (1, 64, 56, 56)        (Type: BatchNorm2d)     | conv2_block3_2_bn            -> (1, 56, 56, 64)        (Type: BatchNormalization)
31.    layer1.2.relu                -> (1, 64, 56, 56)        (Type: ReLU)            | conv2_block3_2_relu          -> (1, 56, 56, 64)        (Type: Activation)      
32.    layer1.2.conv3               -> (1, 256, 56, 56)       (Type: Conv2d)          | conv2_block3_3_conv          -> (1, 56, 56, 256)       (Type: Conv2D)          
33.    layer1.2.bn3                 -> (1, 256, 56, 56)       (Type: BatchNorm2d)     | conv2_block3_3_bn            -> (1, 56, 56, 256)       (Type: BatchNormalization)
34.    layer1.2.relu                -> (1, 256, 56, 56)       (Type: ReLU)            | conv2_block3_out             -> (1, 56, 56, 256)       (Type: Activation)      
35.    layer2.0.conv1               -> (1, 128, 56, 56)       (Type: Conv2d)          | conv3_block1_1_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
36.    layer2.0.bn1                 -> (1, 128, 56, 56)       (Type: BatchNorm2d)     | conv3_block1_1_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
37.    layer2.0.relu                -> (1, 128, 56, 56)       (Type: ReLU)            | conv3_block1_1_relu          -> (1, 28, 28, 128)       (Type: Activation)      
38.    layer2.0.conv2               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block1_2_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
39.    layer2.0.bn2                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block1_2_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
40.    layer2.0.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block1_2_relu          -> (1, 28, 28, 128)       (Type: Activation)      
41.    layer2.0.conv3               -> (1, 512, 28, 28)       (Type: Conv2d)          | conv3_block1_3_conv          -> (1, 28, 28, 512)       (Type: Conv2D)          
42.    layer2.0.bn3                 -> (1, 512, 28, 28)       (Type: BatchNorm2d)     | conv3_block1_3_bn            -> (1, 28, 28, 512)       (Type: BatchNormalization)
43.    layer2.0.downsample.0        -> (1, 512, 28, 28)       (Type: Conv2d)          | conv3_block1_0_conv          -> (1, 28, 28, 512)       (Type: Conv2D)          
44.    layer2.0.downsample.1        -> (1, 512, 28, 28)       (Type: BatchNorm2d)     | conv3_block1_0_bn            -> (1, 28, 28, 512)       (Type: BatchNormalization)
45.    layer2.0.relu                -> (1, 512, 28, 28)       (Type: ReLU)            | conv3_block1_out             -> (1, 28, 28, 512)       (Type: Activation)      
46.    layer2.1.conv1               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block2_1_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
47.    layer2.1.bn1                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block2_1_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
48.    layer2.1.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block2_1_relu          -> (1, 28, 28, 128)       (Type: Activation)      
49.    layer2.1.conv2               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block2_2_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
50.    layer2.1.bn2                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block2_2_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
51.    layer2.1.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block2_2_relu          -> (1, 28, 28, 128)       (Type: Activation)      
52.    layer2.1.conv3               -> (1, 512, 28, 28)       (Type: Conv2d)          | conv3_block2_3_conv          -> (1, 28, 28, 512)       (Type: Conv2D)          
53.    layer2.1.bn3                 -> (1, 512, 28, 28)       (Type: BatchNorm2d)     | conv3_block2_3_bn            -> (1, 28, 28, 512)       (Type: BatchNormalization)
54.    layer2.1.relu                -> (1, 512, 28, 28)       (Type: ReLU)            | conv3_block2_out             -> (1, 28, 28, 512)       (Type: Activation)      
55.    layer2.2.conv1               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block3_1_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
56.    layer2.2.bn1                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block3_1_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
57.    layer2.2.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block3_1_relu          -> (1, 28, 28, 128)       (Type: Activation)      
58.    layer2.2.conv2               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block3_2_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
59.    layer2.2.bn2                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block3_2_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
60.    layer2.2.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block3_2_relu          -> (1, 28, 28, 128)       (Type: Activation)      
61.    layer2.2.conv3               -> (1, 512, 28, 28)       (Type: Conv2d)          | conv3_block3_3_conv          -> (1, 28, 28, 512)       (Type: Conv2D)          
62.    layer2.2.bn3                 -> (1, 512, 28, 28)       (Type: BatchNorm2d)     | conv3_block3_3_bn            -> (1, 28, 28, 512)       (Type: BatchNormalization)
63.    layer2.2.relu                -> (1, 512, 28, 28)       (Type: ReLU)            | conv3_block3_out             -> (1, 28, 28, 512)       (Type: Activation)      
64.    layer2.3.conv1               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block4_1_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
65.    layer2.3.bn1                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block4_1_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
66.    layer2.3.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block4_1_relu          -> (1, 28, 28, 128)       (Type: Activation)      
67.    layer2.3.conv2               -> (1, 128, 28, 28)       (Type: Conv2d)          | conv3_block4_2_conv          -> (1, 28, 28, 128)       (Type: Conv2D)          
68.    layer2.3.bn2                 -> (1, 128, 28, 28)       (Type: BatchNorm2d)     | conv3_block4_2_bn            -> (1, 28, 28, 128)       (Type: BatchNormalization)
69.    layer2.3.relu                -> (1, 128, 28, 28)       (Type: ReLU)            | conv3_block4_2_relu          -> (1, 28, 28, 128)       (Type: Activation)      
70.    layer2.3.conv3               -> (1, 512, 28, 28)       (Type: Conv2d)          | conv3_block4_3_conv          -> (1, 28, 28, 512)       (Type: Conv2D)          
71.    layer2.3.bn3                 -> (1, 512, 28, 28)       (Type: BatchNorm2d)     | conv3_block4_3_bn            -> (1, 28, 28, 512)       (Type: BatchNormalization)
72.    layer2.3.relu                -> (1, 512, 28, 28)       (Type: ReLU)            | conv3_block4_out             -> (1, 28, 28, 512)       (Type: Activation)      
73.    layer3.0.conv1               -> (1, 256, 28, 28)       (Type: Conv2d)          | conv4_block1_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
74.    layer3.0.bn1                 -> (1, 256, 28, 28)       (Type: BatchNorm2d)     | conv4_block1_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
75.    layer3.0.relu                -> (1, 256, 28, 28)       (Type: ReLU)            | conv4_block1_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
76.    layer3.0.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block1_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
77.    layer3.0.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block1_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
78.    layer3.0.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block1_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
79.    layer3.0.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block1_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
80.    layer3.0.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block1_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
81.    layer3.0.downsample.0        -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block1_0_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
82.    layer3.0.downsample.1        -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block1_0_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
83.    layer3.0.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block1_out             -> (1, 14, 14, 1024)      (Type: Activation)      
84.    layer3.1.conv1               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block2_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
85.    layer3.1.bn1                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block2_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
86.    layer3.1.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block2_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
87.    layer3.1.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block2_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
88.    layer3.1.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block2_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
89.    layer3.1.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block2_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
90.    layer3.1.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block2_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
91.    layer3.1.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block2_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
92.    layer3.1.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block2_out             -> (1, 14, 14, 1024)      (Type: Activation)      
93.    layer3.2.conv1               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block3_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
94.    layer3.2.bn1                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block3_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
95.    layer3.2.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block3_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
96.    layer3.2.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block3_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
97.    layer3.2.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block3_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
98.    layer3.2.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block3_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
99.    layer3.2.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block3_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
100.   layer3.2.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block3_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
101.   layer3.2.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block3_out             -> (1, 14, 14, 1024)      (Type: Activation)      
102.   layer3.3.conv1               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block4_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
103.   layer3.3.bn1                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block4_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
104.   layer3.3.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block4_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
105.   layer3.3.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block4_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
106.   layer3.3.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block4_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
107.   layer3.3.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block4_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
108.   layer3.3.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block4_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
109.   layer3.3.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block4_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
110.   layer3.3.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block4_out             -> (1, 14, 14, 1024)      (Type: Activation)      
111.   layer3.4.conv1               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block5_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
112.   layer3.4.bn1                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block5_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
113.   layer3.4.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block5_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
114.   layer3.4.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block5_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
115.   layer3.4.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block5_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
116.   layer3.4.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block5_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
117.   layer3.4.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block5_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
118.   layer3.4.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block5_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
119.   layer3.4.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block5_out             -> (1, 14, 14, 1024)      (Type: Activation)      
120.   layer3.5.conv1               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block6_1_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
121.   layer3.5.bn1                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block6_1_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
122.   layer3.5.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block6_1_relu          -> (1, 14, 14, 256)       (Type: Activation)      
123.   layer3.5.conv2               -> (1, 256, 14, 14)       (Type: Conv2d)          | conv4_block6_2_conv          -> (1, 14, 14, 256)       (Type: Conv2D)          
124.   layer3.5.bn2                 -> (1, 256, 14, 14)       (Type: BatchNorm2d)     | conv4_block6_2_bn            -> (1, 14, 14, 256)       (Type: BatchNormalization)
125.   layer3.5.relu                -> (1, 256, 14, 14)       (Type: ReLU)            | conv4_block6_2_relu          -> (1, 14, 14, 256)       (Type: Activation)      
126.   layer3.5.conv3               -> (1, 1024, 14, 14)      (Type: Conv2d)          | conv4_block6_3_conv          -> (1, 14, 14, 1024)      (Type: Conv2D)          
127.   layer3.5.bn3                 -> (1, 1024, 14, 14)      (Type: BatchNorm2d)     | conv4_block6_3_bn            -> (1, 14, 14, 1024)      (Type: BatchNormalization)
128.   layer3.5.relu                -> (1, 1024, 14, 14)      (Type: ReLU)            | conv4_block6_out             -> (1, 14, 14, 1024)      (Type: Activation)      
129.   layer4.0.conv1               -> (1, 512, 14, 14)       (Type: Conv2d)          | conv5_block1_1_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
130.   layer4.0.bn1                 -> (1, 512, 14, 14)       (Type: BatchNorm2d)     | conv5_block1_1_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
131.   layer4.0.relu                -> (1, 512, 14, 14)       (Type: ReLU)            | conv5_block1_1_relu          -> (1, 7, 7, 512)         (Type: Activation)      
132.   layer4.0.conv2               -> (1, 512, 7, 7)         (Type: Conv2d)          | conv5_block1_2_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
133.   layer4.0.bn2                 -> (1, 512, 7, 7)         (Type: BatchNorm2d)     | conv5_block1_2_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
134.   layer4.0.relu                -> (1, 512, 7, 7)         (Type: ReLU)            | conv5_block1_2_relu          -> (1, 7, 7, 512)         (Type: Activation)      
135.   layer4.0.conv3               -> (1, 2048, 7, 7)        (Type: Conv2d)          | conv5_block1_3_conv          -> (1, 7, 7, 2048)        (Type: Conv2D)          
136.   layer4.0.bn3                 -> (1, 2048, 7, 7)        (Type: BatchNorm2d)     | conv5_block1_3_bn            -> (1, 7, 7, 2048)        (Type: BatchNormalization)
137.   layer4.0.downsample.0        -> (1, 2048, 7, 7)        (Type: Conv2d)          | conv5_block1_0_conv          -> (1, 7, 7, 2048)        (Type: Conv2D)          
138.   layer4.0.downsample.1        -> (1, 2048, 7, 7)        (Type: BatchNorm2d)     | conv5_block1_0_bn            -> (1, 7, 7, 2048)        (Type: BatchNormalization)
139.   layer4.0.relu                -> (1, 2048, 7, 7)        (Type: ReLU)            | conv5_block1_out             -> (1, 7, 7, 2048)        (Type: Activation)      
140.   layer4.1.conv1               -> (1, 512, 7, 7)         (Type: Conv2d)          | conv5_block2_1_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
141.   layer4.1.bn1                 -> (1, 512, 7, 7)         (Type: BatchNorm2d)     | conv5_block2_1_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
142.   layer4.1.relu                -> (1, 512, 7, 7)         (Type: ReLU)            | conv5_block2_1_relu          -> (1, 7, 7, 512)         (Type: Activation)      
143.   layer4.1.conv2               -> (1, 512, 7, 7)         (Type: Conv2d)          | conv5_block2_2_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
144.   layer4.1.bn2                 -> (1, 512, 7, 7)         (Type: BatchNorm2d)     | conv5_block2_2_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
145.   layer4.1.relu                -> (1, 512, 7, 7)         (Type: ReLU)            | conv5_block2_2_relu          -> (1, 7, 7, 512)         (Type: Activation)      
146.   layer4.1.conv3               -> (1, 2048, 7, 7)        (Type: Conv2d)          | conv5_block2_3_conv          -> (1, 7, 7, 2048)        (Type: Conv2D)          
147.   layer4.1.bn3                 -> (1, 2048, 7, 7)        (Type: BatchNorm2d)     | conv5_block2_3_bn            -> (1, 7, 7, 2048)        (Type: BatchNormalization)
148.   layer4.1.relu                -> (1, 2048, 7, 7)        (Type: ReLU)            | conv5_block2_out             -> (1, 7, 7, 2048)        (Type: Activation)      
149.   layer4.2.conv1               -> (1, 512, 7, 7)         (Type: Conv2d)          | conv5_block3_1_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
150.   layer4.2.bn1                 -> (1, 512, 7, 7)         (Type: BatchNorm2d)     | conv5_block3_1_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
151.   layer4.2.relu                -> (1, 512, 7, 7)         (Type: ReLU)            | conv5_block3_1_relu          -> (1, 7, 7, 512)         (Type: Activation)      
152.   layer4.2.conv2               -> (1, 512, 7, 7)         (Type: Conv2d)          | conv5_block3_2_conv          -> (1, 7, 7, 512)         (Type: Conv2D)          
153.   layer4.2.bn2                 -> (1, 512, 7, 7)         (Type: BatchNorm2d)     | conv5_block3_2_bn            -> (1, 7, 7, 512)         (Type: BatchNormalization)
154.   layer4.2.relu                -> (1, 512, 7, 7)         (Type: ReLU)            | conv5_block3_2_relu          -> (1, 7, 7, 512)         (Type: Activation)      
155.   layer4.2.conv3               -> (1, 2048, 7, 7)        (Type: Conv2d)          | conv5_block3_3_conv          -> (1, 7, 7, 2048)        (Type: Conv2D)          
156.   layer4.2.bn3                 -> (1, 2048, 7, 7)        (Type: BatchNorm2d)     | conv5_block3_3_bn            -> (1, 7, 7, 2048)        (Type: BatchNormalization)
157.   layer4.2.relu                -> (1, 2048, 7, 7)        (Type: ReLU)            | conv5_block3_out             -> (1, 7, 7, 2048)        (Type: Activation)      
158.   avgpool                      -> (1, 2048, 1, 1)        (Type: AdaptiveAvgPool2d)| N/A                          -> N/A                    (Type: N/A)             
159.   fc                           -> (1, 1000)              (Type: Linear)          | N/A                          -> N/A                    (Type: N/A)             
